{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "curious-simpson",
   "metadata": {},
   "source": [
    "# Sentence Extraction from PMC articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "frequent-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lexas.sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "realistic-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /mnt/a/nxml/data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "perfect-material",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:03<00:00,  1.83it/s]\n"
     ]
    }
   ],
   "source": [
    "lexas.sentence.result_extraction(#article_dir=\"./articles/\",\n",
    "                                 #output=\"./data/result_sections.txt\"\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-administration",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10731 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 2023-03-03 19:47:48.420536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2512/10731 [05:01<22:09,  6.18it/s]  "
     ]
    }
   ],
   "source": [
    "#Result extraction\n",
    "import datetime\n",
    "for i in range(10):\n",
    "    print(\"i\",datetime.datetime.now())\n",
    "    lexas.sentence.result_extraction(article_dir=\"/mnt/a/nxml/data/PMC00{}xxxxxx/\".format(i),\n",
    "                                 output=\"/mnt/a/nxml/data2/results_{}.txt\".format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexas.sentence.result_extraction(article_dir=\"/mnt/a/nxml/data/PMC000xxxxxx/\",\n",
    "                                 output=\"/mnt/a/nxml/data/results2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "induced-pierce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [00:00, 1350.96it/s]\n"
     ]
    }
   ],
   "source": [
    "#Masking gene terms and experiments\n",
    "lexas.sentence.mask_gene_experiment()#input=\"./data/result_sections.txt\",output=\"./data/masked_sentences.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "respected-chicago",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'Repository/biobert/tokenizer_biobert'. Make sure that:\n\n- 'Repository/biobert/tokenizer_biobert' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'Repository/biobert/tokenizer_biobert' is the correct path to a directory containing relevant tokenizer files\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-4a31ff8aab49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./data/masked_sentences_bert.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlexas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelation_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/lexas/lexas/relation_extraction.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(device, input, output, threshold)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Repository/biobert/tokenizer_biobert'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     model = BertForSequenceClassification.from_pretrained(\n\u001b[1;32m     10\u001b[0m          \u001b[0;34m\"Repository/biobert/model_biobert-finetuning\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1775\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing relevant tokenizer files\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m             )\n\u001b[0;32m-> 1777\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'Repository/biobert/tokenizer_biobert'. Make sure that:\n\n- 'Repository/biobert/tokenizer_biobert' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'Repository/biobert/tokenizer_biobert' is the correct path to a directory containing relevant tokenizer files\n\n"
     ]
    }
   ],
   "source": [
    "#Relation extraction using bio-BERT\n",
    "import lexas.relation_extraction\n",
    "import torch\n",
    "device=torch.device(\"cpu\")#device=torch.device(\"cuda\")\n",
    "input_file=\"./data/masked_sentences.txt\"\n",
    "output_file=\"./data/masked_sentences_bert.txt\"\n",
    "\n",
    "lexas.relation_extraction.predict(device,input_file,output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-panel",
   "metadata": {},
   "source": [
    "# Prediction model for genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "satisfied-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lexas.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "qualified-southwest",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lexas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f26b619d2535>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./data/masked_sentences_bert.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./data/experiments_for_xgboost.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlexas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lexas' is not defined"
     ]
    }
   ],
   "source": [
    "#Extraction of experment_context\n",
    "input_file=\"./data/masked_sentences_bert.txt\"\n",
    "output_file=\"./data/experiments_for_xgboost.csv\"\n",
    "lexas.prediction.experiment_context(input_file,output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mobile-associate",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading categorical features...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Repository/feature/categorical_feature.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dcee79fd7b2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Loading features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlexas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msymbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlexas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcat_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Chromosome'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GO'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MGI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HPO'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'OMIM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'iRefIndex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Localization'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'WebSter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/lexas/lexas/prediction.py\u001b[0m in \u001b[0;36mfeature_load\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#Loading dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading categorical features...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./Repository/feature/categorical_feature.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Repository/feature/categorical_feature.txt'"
     ]
    }
   ],
   "source": [
    "#Loading features\n",
    "lexas.prediction.feature_load()\n",
    "symbols = lexas.prediction.symbols\n",
    "\n",
    "cat_use = ['Chromosome', 'GO', 'MGI', 'HPO', 'OMIM', 'TF', 'iRefIndex', 'Localization', 'WebSter']\n",
    "num_use = ['Tissue_expression', 'Cancer_expression', 'DepMap', 'Word2Vec']\n",
    "\n",
    "feature_list,all_cat,cat_num,all_num = lexas.prediction.choose_feature(cat_use,num_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dense-brighton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing CSR matrix...  Done\n",
      "Constructing CSR matrix...  Done\n"
     ]
    }
   ],
   "source": [
    "#Constracting csr matrix\n",
    "path_to_csv =\"./data/experiments_for_xgboost.csv\"\n",
    "\n",
    "#Experiment tuples for training \n",
    "#from 2010-2020\n",
    "#Return tuples of positive examples and 10 times the number of negative examples\n",
    "posi_tuple,nega_tuple = lexas.prediction.make_tuple(path_to_csv,2010,2020,sampling=None)\n",
    "\n",
    "#Experiment tuples for validation\n",
    "#from 2021-2023\n",
    "posi_tuple_dev,nega_tuple_dev = lexas.prediction.make_tuple(path_to_csv,2021,2023,sampling=None)\n",
    "\n",
    "#Constructing CSR sparse matrix for training\n",
    "X,y = lexas.prediction.make_csr(posi_tuple,nega_tuple,all_cat,cat_num,all_num)\n",
    "X_dev,y_dev = lexas.prediction.make_csr(posi_tuple_dev,nega_tuple_dev,all_cat,cat_num,all_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vertical-allergy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.6839728564638003\n"
     ]
    }
   ],
   "source": [
    "#Training a SVM model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import pickle\n",
    "\n",
    "reg = 0.001\n",
    "lrn = SGDClassifier(loss='hinge', alpha=reg,penalty='l2')\n",
    "lrn.fit(X,y)\n",
    "calibrator = CalibratedClassifierCV(lrn, cv='prefit')\n",
    "model=calibrator.fit(X,y)\n",
    "pickle.dump(model, open(\"./model/svm_{}.pickle\".format(reg), \"wb\"))\n",
    "\n",
    "#AUC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred = model.predict_proba(X_dev)[:,1]\n",
    "print(\"AUC:\",roc_auc_score(y_dev,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-philosophy",
   "metadata": {},
   "source": [
    "# Gene prediction for the next experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "excellent-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before running, please load the features\n",
    "import os\n",
    "import lexas.prediction\n",
    "import pickle\n",
    "model = pickle.load(open(\"./model/svm_0.001.pickle\",\"rb\"))\n",
    "models={\"svm-0.01\":model}\n",
    "query=\"CEP57\"\n",
    "\n",
    "scores = lexas.prediction.scoring(query,models,all_cat,cat_num,all_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "younger-finding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>svm-0.01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1BG</td>\n",
       "      <td>0.137148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1BG-AS1</td>\n",
       "      <td>0.042312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1CF</td>\n",
       "      <td>0.061554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2M</td>\n",
       "      <td>0.024529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2ML1</td>\n",
       "      <td>0.013632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22938</th>\n",
       "      <td>PEPN</td>\n",
       "      <td>0.042129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22939</th>\n",
       "      <td>PYK</td>\n",
       "      <td>0.042086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22940</th>\n",
       "      <td>TAX</td>\n",
       "      <td>0.148369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22941</th>\n",
       "      <td>TMPRSS3</td>\n",
       "      <td>0.015149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22942</th>\n",
       "      <td>XAGE1E</td>\n",
       "      <td>0.041072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22943 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Symbol  svm-0.01\n",
       "0          A1BG  0.137148\n",
       "1      A1BG-AS1  0.042312\n",
       "2          A1CF  0.061554\n",
       "3           A2M  0.024529\n",
       "4         A2ML1  0.013632\n",
       "...         ...       ...\n",
       "22938      PEPN  0.042129\n",
       "22939       PYK  0.042086\n",
       "22940       TAX  0.148369\n",
       "22941   TMPRSS3  0.015149\n",
       "22942    XAGE1E  0.041072\n",
       "\n",
       "[22943 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Result\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(scores)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-groove",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
