{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "curious-simpson",
   "metadata": {},
   "source": [
    "# Sentence Extraction from PMC articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "measured-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download 10 sample articles for demonstration\n",
    "!python3 download_pmc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "frequent-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lexas.sentence\n",
    "import lexas.relation_extraction\n",
    "import torch\n",
    "\n",
    "# Define the device to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "demographic-greek",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ac3661d88743a6a2128b688e23b54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Extracting result sections from the articles\n",
    "# The 'extract_results' function is used to extract sections of results from articles.\n",
    "lexas.sentence.extract_results(\n",
    "    article_dir=\"./articles/\",\n",
    "    output_file=\"./data/result_sections.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "signal-annex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dictionaries...\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41fd7a615725471d863584408ce2f493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: Masking gene terms and experiments\n",
    "# The 'mask_gene_experiment' function is used to replace gene terms and experiments with MASK tokens in the text.\n",
    "lexas.sentence.mask_gene_experiment(\n",
    "    input_file_path=\"./data/result_sections.txt\",\n",
    "    output_file_path=\"./data/masked_sentences.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "actual-cookbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1517it [03:21,  7.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Relation extraction using BioBERT\n",
    "# The 'predict' function is used to predict relations using BioBERT model on the masked sentences.\n",
    "lexas.relation_extraction.predict(\n",
    "    device=device,\n",
    "    input_filepath=\"./data/masked_sentences.txt\",\n",
    "    output_filepath=\"./data/masked_sentences_bert.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-panel",
   "metadata": {},
   "source": [
    "# Prediction model for genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "economic-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lexas.prediction\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eleven-uruguay",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "824it [00:00, 148841.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extracting context from experiments\n",
    "# The 'extract_context_from_experiments' function processes the BioBERT predictions\n",
    "# and extracts the context in which each experiment mention was made.\n",
    "lexas.prediction.extract_context_from_experiments(\n",
    "    input_file=\"./data/masked_sentences_bert.txt\",\n",
    "    output_file=\"./data/experiments_for_xgboost.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "noble-contrast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading categorical features...\n",
      "Loading numerical features...\n",
      "Loading string11_rwr.txt...\n",
      "Loading funcoup5_rwr.txt...\n",
      "Loading gosemsim.txt...\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Loading feature data\n",
    "# The 'feature_load' function is used to load feature data from various resources.\n",
    "lexas.prediction.feature_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "valuable-synthesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of features:  ['10p', '10q', '11p', '11q', '12p', '12q', '13q', '14p', '14q', '15q']\n",
      "\n",
      "Features assigned to a gene:  ['10q', 'GO:0046686', 'GO:0065003', 'GO:0005634', 'GO:0030261', 'GO:0004674', 'GO:0000086', 'GO:0007098', 'GO:0060045', 'GO:0006281']\n",
      "\n",
      "Numerical features assigned to a gene:  dict_keys(['Tissue_expression', 'Cancer_expression', 'DepMap', 'Word2Vec'])\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Selecting features\n",
    "# The 'select_features' function is used to select the features to be used in the model.\n",
    "cat_use = ['Chromosome', 'GO', 'MGI', 'HPO', 'OMIM', 'TF', 'iRefIndex', 'Localization', 'WebSter']\n",
    "num_use = ['Tissue_expression', 'Cancer_expression', 'DepMap', 'Word2Vec']\n",
    "plus = [\"String\",\"Funcoup\",\"GOSemSim\"]\n",
    "feature_list, gene_cat, gene_num = lexas.prediction.select_features(cat_use, num_use)\n",
    "\n",
    "# Print feature information\n",
    "print(\"List of features: \", feature_list[:10])\n",
    "print(\"\\nFeatures assigned to a gene: \", gene_cat[\"CDK1\"][:10])\n",
    "print(\"\\nNumerical features assigned to a gene: \", gene_num.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "union-suggestion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "798it [00:00, 410429.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing CSR matrix...  Done\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Constructing the CSR matrix\n",
    "# The 'construct_csr_matrix' function is used to transform the data into a format that can be processed by the XGBoost model.\n",
    "path_to_csv=\"./data/experiments_for_xgboost.csv\"\n",
    "posi_tuple, nega_tuple = lexas.prediction.generate_experiment_tuples(path_to_csv, 1990, 2018, negative_sampling=3)\n",
    "X, y = lexas.prediction.construct_csr_matrix(posi_tuple, nega_tuple, gene_cat, feature_list, gene_num, additional_features=plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "developmental-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train-Test split\n",
    "# The train_test_split function is used to split the data into training and testing sets for model training and evaluation.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "patient-double",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:05:45] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-logloss:0.68599\n",
      "[1]\tvalidation_0-logloss:0.67955\n",
      "[2]\tvalidation_0-logloss:0.67349\n",
      "[3]\tvalidation_0-logloss:0.66691\n",
      "[4]\tvalidation_0-logloss:0.66119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.6/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\tvalidation_0-logloss:0.65610\n",
      "[6]\tvalidation_0-logloss:0.65109\n",
      "[7]\tvalidation_0-logloss:0.64655\n",
      "[8]\tvalidation_0-logloss:0.64146\n",
      "[9]\tvalidation_0-logloss:0.63839\n",
      "[10]\tvalidation_0-logloss:0.63651\n",
      "[11]\tvalidation_0-logloss:0.63242\n",
      "[12]\tvalidation_0-logloss:0.63077\n",
      "[13]\tvalidation_0-logloss:0.62684\n",
      "[14]\tvalidation_0-logloss:0.62539\n",
      "[15]\tvalidation_0-logloss:0.62055\n",
      "[16]\tvalidation_0-logloss:0.61928\n",
      "[17]\tvalidation_0-logloss:0.61469\n",
      "[18]\tvalidation_0-logloss:0.61359\n",
      "[19]\tvalidation_0-logloss:0.60924\n",
      "[20]\tvalidation_0-logloss:0.60850\n",
      "[21]\tvalidation_0-logloss:0.60439\n",
      "[22]\tvalidation_0-logloss:0.60355\n",
      "[23]\tvalidation_0-logloss:0.59964\n",
      "[24]\tvalidation_0-logloss:0.59894\n",
      "[25]\tvalidation_0-logloss:0.59524\n",
      "[26]\tvalidation_0-logloss:0.59467\n",
      "[27]\tvalidation_0-logloss:0.59115\n",
      "[28]\tvalidation_0-logloss:0.59070\n",
      "[29]\tvalidation_0-logloss:0.58941\n",
      "[30]\tvalidation_0-logloss:0.58615\n",
      "[31]\tvalidation_0-logloss:0.58582\n",
      "[32]\tvalidation_0-logloss:0.58272\n",
      "[33]\tvalidation_0-logloss:0.58263\n",
      "[34]\tvalidation_0-logloss:0.58095\n",
      "[35]\tvalidation_0-logloss:0.57853\n",
      "[36]\tvalidation_0-logloss:0.57600\n",
      "[37]\tvalidation_0-logloss:0.57371\n",
      "[38]\tvalidation_0-logloss:0.57364\n",
      "[39]\tvalidation_0-logloss:0.57134\n",
      "[40]\tvalidation_0-logloss:0.56991\n",
      "[41]\tvalidation_0-logloss:0.56784\n",
      "[42]\tvalidation_0-logloss:0.56570\n",
      "[43]\tvalidation_0-logloss:0.56377\n",
      "[44]\tvalidation_0-logloss:0.56388\n",
      "[45]\tvalidation_0-logloss:0.56191\n",
      "[46]\tvalidation_0-logloss:0.56011\n",
      "[47]\tvalidation_0-logloss:0.55955\n",
      "[48]\tvalidation_0-logloss:0.55788\n",
      "[49]\tvalidation_0-logloss:0.55605\n",
      "[50]\tvalidation_0-logloss:0.55560\n",
      "[51]\tvalidation_0-logloss:0.55407\n",
      "[52]\tvalidation_0-logloss:0.55406\n",
      "[53]\tvalidation_0-logloss:0.55334\n",
      "[54]\tvalidation_0-logloss:0.55343\n",
      "[55]\tvalidation_0-logloss:0.55278\n",
      "[56]\tvalidation_0-logloss:0.55321\n",
      "[57]\tvalidation_0-logloss:0.55160\n",
      "[58]\tvalidation_0-logloss:0.55211\n",
      "[59]\tvalidation_0-logloss:0.55189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./model/xgboost.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Model Training\n",
    "# The XGBClassifier is used to train a model on the data. The model is then saved using the joblib library.\n",
    "model = xgb.XGBClassifier(\n",
    "    objective= \"binary:logistic\",\n",
    "    alpha=1e-3, \n",
    "    min_child_weight=3,\n",
    "    max_depth=10,\n",
    "    n_estimators=40000,\n",
    "    n_jobs=-1,\n",
    "    eta=0.03\n",
    ")\n",
    "model.fit(X_train, y_train, early_stopping_rounds=3, eval_set=[[X_test, y_test]])\n",
    "\n",
    "# Save the model to a file\n",
    "from joblib import dump\n",
    "model_path = \"./model/xgboost.joblib\"\n",
    "dump(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-relationship",
   "metadata": {},
   "source": [
    "# Gene prediction for the next experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "measured-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lexas.prediction\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "\n",
    "# Step 1: Load model\n",
    "model_name = \"xgboost\"\n",
    "\n",
    "model_filepath = f\"./model/{model_name}.joblib\"\n",
    "\n",
    "if os.path.exists(model_filepath):\n",
    "    model = load(model_filepath)\n",
    "else:\n",
    "    raise Exception(\"Model file does not exist: \" + model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "encouraging-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate scores\n",
    "# The 'generate_scores' function is used to score all genes in relation to the query using the XGBoost model.\n",
    "query = \"CEP152\"\n",
    "scores = lexas.prediction.generate_scores(query, model_name, model, gene_cat, feature_list, gene_num, additional_features=plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chinese-asbestos",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>xgboost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3006</th>\n",
       "      <td>CHD3</td>\n",
       "      <td>0.648184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7154</th>\n",
       "      <td>HIRA</td>\n",
       "      <td>0.648184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2661</th>\n",
       "      <td>CDC42BPA</td>\n",
       "      <td>0.648184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16973</th>\n",
       "      <td>TMEM150C</td>\n",
       "      <td>0.648184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6892</th>\n",
       "      <td>H2AC8</td>\n",
       "      <td>0.648184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3559</th>\n",
       "      <td>CPM</td>\n",
       "      <td>0.648184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6414</th>\n",
       "      <td>GNA11</td>\n",
       "      <td>0.638069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16413</th>\n",
       "      <td>TAS2R45</td>\n",
       "      <td>0.638069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16458</th>\n",
       "      <td>TBC1D3B</td>\n",
       "      <td>0.638069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9200</th>\n",
       "      <td>MAGEA2</td>\n",
       "      <td>0.638069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Symbol   xgboost\n",
       "3006       CHD3  0.648184\n",
       "7154       HIRA  0.648184\n",
       "2661   CDC42BPA  0.648184\n",
       "16973  TMEM150C  0.648184\n",
       "6892      H2AC8  0.648184\n",
       "3559        CPM  0.648184\n",
       "6414      GNA11  0.638069\n",
       "16413   TAS2R45  0.638069\n",
       "16458   TBC1D3B  0.638069\n",
       "9200     MAGEA2  0.638069"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Display result\n",
    "# The result is displayed as a DataFrame sorted by the XGBoost score in descending order.\n",
    "output_dir = f\"./result/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df = pd.DataFrame(scores)\n",
    "df.to_csv(os.path.join(output_dir,f\"{query}.csv\"),index=False)\n",
    "df.sort_values(model_name, ascending=False)[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
